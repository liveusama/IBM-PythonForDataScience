{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google News Scrapper",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEjPX1m7rZy_",
        "colab_type": "text"
      },
      "source": [
        "# **Installing libaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCBgowATil0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "35a4dcc9-2ad4-415f-b1c7-1e6e87d5bc0b"
      },
      "source": [
        "!pip install newspaper3k\n",
        "!pip install GoogleNews"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 4.2MB/s \n",
            "\u001b[?25hCollecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/cf/d0ff82625e53bd245d6173ce6333d190abbfcd94e4c30e54b4e16b474216/tldextract-2.2.3-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 6.3MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, feedparser\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=1e4d4cd80d71a6fb2f64f8201a706f296fa6f4256e24b0b2257b5f6c4bac892c\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=c1cf7d8af0bbb3bfa8a1ce5a9b24784ba5af8dc7fa6c316bb3093c4925bdaa12\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=f25c4dac7b04069d7186f46ac251888c34b16bc1d260399ffef19364d1b6e850\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44939 sha256=89aeee223639708084a59d83e6f8a65d1f6fb17e9ab928febe000781604560f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k feedparser\n",
            "Installing collected packages: requests-file, tldextract, tinysegmenter, cssselect, feedfinder2, jieba3k, feedparser, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.3\n",
            "Collecting GoogleNews\n",
            "  Downloading https://files.pythonhosted.org/packages/65/43/4d5008157ca602aa87ebba28b1c0ac8451b0b3f77f5467de7cec8c88e5f1/GoogleNews-1.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from GoogleNews) (4.6.3)\n",
            "Installing collected packages: GoogleNews\n",
            "Successfully installed GoogleNews-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fysOFNzarjUZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Importing libaries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMS61svnspRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "d56919a2-e6f3-4886-d399-1a4dce5e0a3e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article\n",
        "from newspaper import Config\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTcy6-HCBLU9",
        "colab_type": "text"
      },
      "source": [
        "# **Code**\n",
        "In this notbook I'm automating google news. [Google News](https://news.google.com/)\n",
        "### Requirements to run this code:\n",
        "\n",
        "*   Google Colab GPU otherwise its give runtime error.\n",
        "*   User agent (Everyone has different) [Here is Link to search your own.](https://www.whatismybrowser.com/detect/what-is-my-user-agent)\n",
        "*   Search words list\n",
        "*   Strating and Ending Date\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yY0_XgHi73g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "616a1041-4580-4d34-94bd-36ce7bf00be1"
      },
      "source": [
        "#config will allow us to access the specified url for which we are #not authorized. \n",
        "#Sometimes we may get 403 client error while parsing the link to download the article.\n",
        "search_words = ['child labour in pakistan', 'child abuse in pakistan', 'rape case in pakistan', 'child harassment in pakistan', \"forced to girl in pakistan\", 'sexual harassment with girl in Pakistan']\n",
        "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36' #search your user agent\n",
        "config = Config()\n",
        "config.browser_user_agent = user_agent\n",
        "googlenews=GoogleNews(start='05/01/2018',end='08/27/2020')\n",
        "for word in search_words:\n",
        "    googlenews.search(word)\n",
        "    result=googlenews.result()\n",
        "    df=pd.DataFrame(result)\n",
        "    for i in range(2,20):\n",
        "        googlenews.getpage(i)\n",
        "        result=googlenews.result()\n",
        "        df=pd.DataFrame(result)\n",
        "    list=[]\n",
        "    for ind in df.index:\n",
        "        dict={}\n",
        "        article = Article(df['link'][ind],config=config)\n",
        "        try: #some articles creating problem in (parse) so i use try and except\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            article.nlp()\n",
        "        except:\n",
        "            pass\n",
        "        dict['Date']=df['date'][ind]\n",
        "        dict['Media']=df['media'][ind]\n",
        "        author = article.authors\n",
        "        authors=\"\"\n",
        "        for word in author: #using for loop beacause author return list [] as i converting into string\n",
        "          authors+=word+', '\n",
        "        dict['authors'] = authors\n",
        "        dict['Title']=article.title\n",
        "        dict['Article']=article.text\n",
        "        dict['Summary']=article.summary\n",
        "        keyword=article.keywords\n",
        "        keywords=\"\"\n",
        "        for word in keyword:\n",
        "          keywords+=word+', '\n",
        "        dict['keywords'] = keywords\n",
        "        dict['link'] = article.url\n",
        "        list.append(dict)  \n",
        "    news_df=pd.DataFrame(list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dateutil/parser/_parser.py:1218: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  category=UnknownTimezoneWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naTtfFSu-wA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3e63a138-6cb9-439a-9270-1ec8d20037b2"
      },
      "source": [
        "news_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Media</th>\n",
              "      <th>authors</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article</th>\n",
              "      <th>Summary</th>\n",
              "      <th>keywords</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Jun 5, 2020</td>\n",
              "      <td>Reuters</td>\n",
              "      <td>Zofeen T. Ebrahim, Min Read,</td>\n",
              "      <td>Eight-year-old maid's death spurs calls for ch...</td>\n",
              "      <td>KARACHI (Thomson Reuters Foundation) - The bru...</td>\n",
              "      <td>KARACHI (Thomson Reuters Foundation) - The bru...</td>\n",
              "      <td>rights, work, child, calls, labour, reform, hu...</td>\n",
              "      <td>https://www.reuters.com/article/us-pakistan-ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jun 6, 2020</td>\n",
              "      <td>OpIndia</td>\n",
              "      <td>Opindia Staff, Https, Staff Reporter At Opindia,</td>\n",
              "      <td>Pakistan: 8 year old maid beaten to death for ...</td>\n",
              "      <td>On Sunday, the Pakistan police had reportedly ...</td>\n",
              "      <td>The Ministry had also proposed amendments to t...</td>\n",
              "      <td>social, old, domestic, following, maid, escape...</td>\n",
              "      <td>https://www.opindia.com/2020/06/pakistan-8-yea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mar 26, 2019</td>\n",
              "      <td>The News International</td>\n",
              "      <td>Myra Imran,</td>\n",
              "      <td>MoHR launches survey on child labour in Pakistan</td>\n",
              "      <td>Islamabad : To collect the scientific data of ...</td>\n",
              "      <td>Islamabad : To collect the scientific data of ...</td>\n",
              "      <td>shireen, rights, child, labour, secretary, hum...</td>\n",
              "      <td>https://www.thenews.com.pk/print/448859-mohr-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Oct 22, 2019</td>\n",
              "      <td>The Express Tribune</td>\n",
              "      <td></td>\n",
              "      <td>Child labour in Pakistan</td>\n",
              "      <td>Not all children in Pakistan are lucky enough ...</td>\n",
              "      <td>Though there are laws banning child labour, ch...</td>\n",
              "      <td>provide, work, child, working, labour, need, c...</td>\n",
              "      <td>http://tribune.com.pk/letter/2084376/6-child-l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jun 25, 2019</td>\n",
              "      <td>The News International</td>\n",
              "      <td>Jamila Achakzai,</td>\n",
              "      <td>Report notices Pakistan’s progress against chi...</td>\n",
              "      <td>Islamabad: As Save the Children, an internatio...</td>\n",
              "      <td>Islamabad: As Save the Children, an internatio...</td>\n",
              "      <td>pakistans, child, labour, society, help, organ...</td>\n",
              "      <td>https://www.thenews.com.pk/print/489426-report...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date  ...                                               link\n",
              "0   Jun 5, 2020  ...  https://www.reuters.com/article/us-pakistan-ch...\n",
              "1   Jun 6, 2020  ...  https://www.opindia.com/2020/06/pakistan-8-yea...\n",
              "2  Mar 26, 2019  ...  https://www.thenews.com.pk/print/448859-mohr-l...\n",
              "3  Oct 22, 2019  ...  http://tribune.com.pk/letter/2084376/6-child-l...\n",
              "4  Jun 25, 2019  ...  https://www.thenews.com.pk/print/489426-report...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OA-MDMAMIji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_df.to_excel(\"articles1.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
